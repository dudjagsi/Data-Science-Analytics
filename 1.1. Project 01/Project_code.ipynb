{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Data Acquisition\n",
    "\n",
    "Install the following necessary packages\n",
    "\n",
    "#pip install pandas\n",
    "#pip install numpy\n",
    "#pip install scipy\n",
    "#pip install yahoo_finance_api2\n",
    "#pip install pandas-datareader\n",
    "#pip install ta\n",
    "#pip install seaborn\n",
    "#pip install sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing tickers/Symbols field from the scrapped stockdata\n",
    "\n",
    "df = pd.read_csv('/home/nielit/Desktop/datasample.csv',header=None)\n",
    "df.columns=[\"ticker\"]\n",
    "df.head()\n",
    "\n",
    "len(df.ticker)\n",
    "\n",
    "len(df.ticker.unique())\n",
    "\n",
    "# Check for duplicate values\n",
    "\n",
    "df_dup = df[df.duplicated()]\n",
    "df_dup\n",
    "\n",
    "# Drop the duplicate values\n",
    "\n",
    "df = df.drop(df.index[300])\n",
    "\n",
    "tic = list(df.ticker)\n",
    "\n",
    "# Import necessary packages\n",
    "\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Collecting the stock market data from open source yahoo finance API for the following stock tickers for past 2 years\n",
    "\n",
    "data = list()\n",
    "for i in tqdm(tic):\n",
    "    try:\n",
    "        data.append(pdr.get_data_yahoo(symbols=i, start=datetime(2018, 1, 2), end=datetime(2019, 12, 31)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    data[i]['Symbol'] = tic[i]\n",
    "\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "for i in np.arange(1,len(data)):\n",
    "    df = df.append(data[i])\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "Our required dataframe df looks something like this..\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Saving the dataframe in local\n",
    "\n",
    "df.to_csv(\"/home/nielit/Desktop/StockData2.csv\")\n",
    "\n",
    "# Loading the dataframe back from local \n",
    "\n",
    "df_stock = pd.read_csv(\"/home/nielit/Desktop/StockData2.csv\")\n",
    "\n",
    "# The stock data is in the exact format to store in a database\n",
    "\n",
    "df_stock.info()\n",
    "\n",
    "# Using MongoDB as my database\n",
    "\n",
    "from pymongo import MongoClient \n",
    "from random import randint\n",
    "try: \n",
    "    client_mongo  = MongoClient() \n",
    "    print(\"Connected successfully!!!\") \n",
    "except:   \n",
    "    print(\"Could not connect to MongoDB\") \n",
    "\n",
    "# Creating a database db and collection stock\n",
    "\n",
    "db = client_mongo.db_6\n",
    "collection = db.stock\n",
    "\n",
    "### Importing the dataset into database using MongoClient\n",
    "\n",
    "for i in df_stock.values:\n",
    "    collection.insert_one({\"Date\":i[0],\"High\":i[1],\"Low\":i[2],\"Open\":i[3],\"Close\":i[4],\"Volume\":i[5],\"Adj Close\":i[6],\"Symbol\":i[7]})\n",
    "\n",
    "The stock data is now stored in MongoDB\n",
    "\n",
    "### Exporting the dataset from MongoClient back here\n",
    "\n",
    "df = pd.DataFrame(list(db.stock.find()))\n",
    "df.head()\n",
    "\n",
    "# Reformatting the data back into our desired form from MongoDB\n",
    "\n",
    "df = df.drop(['_id'],axis=1)\n",
    "df = df[['Date','High','Low','Open','Close','Volume','Adj Close','Symbol']]\n",
    "df = df.set_index('Date')\n",
    "df.head()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r\"C:\\Users\\JAGANNATH\\Desktop\\Project\\Project\\StockData2.csv\")\n",
    "df = df.sort_values(by='Date')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index(['Date'])\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "print(\"There are {} Symbols with obervations over {} days.\".format(df.Symbol.unique().size, df.index.unique().size))\n",
    "\n",
    "# Check for null values\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "Converting the End Of Day data into five seperate time series data frames Open, High, Low, Close and Volume (OHLCV).\n",
    "\n",
    "open_val = df[[\"Symbol\", \"Open\"]].pivot(columns = \"Symbol\", values = \"Open\")\n",
    "high_val = df[[\"Symbol\", \"High\"]].pivot(columns = \"Symbol\", values = \"High\")\n",
    "low_val = df[[\"Symbol\", \"Low\"]].pivot(columns = \"Symbol\", values = \"Low\")\n",
    "close_val = df[[\"Symbol\", \"Close\"]].pivot(columns = \"Symbol\", values = \"Close\")\n",
    "volume = df[[\"Symbol\", \"Volume\"]].pivot(columns = \"Symbol\", values = \"Volume\")\n",
    "\n",
    "# Open value dataframe\n",
    "\n",
    "open_val.head()\n",
    "\n",
    "# Close value dataframe\n",
    "\n",
    "close_val.head()\n",
    "\n",
    "close_val.info()\n",
    "\n",
    "We will create a dataframe containing the future close returns at time t, since we are predicting the close returns of next day value.\n",
    "\n",
    "next_val = (close_val.shift(-1) / close_val - 1)\n",
    "\n",
    "# The last day return looks something like this\n",
    "\n",
    "next_val.tail()\n",
    "\n",
    "We will create a data frame containing close returns of the current day, which is calculated with respect to the close price of the previous day.\n",
    "\n",
    "ret_val = (close_val / close_val.shift(1)) - 1\n",
    "\n",
    "# The current day return looks something like this\n",
    "\n",
    "ret_val.head()\n",
    "\n",
    "# Dataframe with ratio of high/low for each symbols\n",
    "\n",
    "hl_val = high_val / low_val\n",
    "hl_val.head()\n",
    "\n",
    "## Data Pre-pocessing\n",
    "\n",
    "Missing values in the close_val dataframe occurs since not all the Symbols contain information from the start date. These missing values may also occur if trade doesn't happen on that day.\n",
    "Let's check the non-missing values.\n",
    "\n",
    "# Histogram for non-missing values for 488 days\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "hist = close_val.notna().sum().hist(bins=10)\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Symbols\")\n",
    "plt.title(\"Non-missing values in Symbols\")\n",
    "print(\"There are {} symbols with full data available.\".format(close_val.columns[(close_val.notna().sum() == close_val.shape[0]).values].shape[0])) \n",
    "plt.show()\n",
    "\n",
    "close_val.notna().sum()\n",
    "\n",
    "# Lets' take Symbols with more than 400 non-missing values\n",
    "\n",
    "valid_sym = close_val.columns[(close_val.notna().sum() >= 400).values]\n",
    "open_val = open_val[valid_sym]\n",
    "high_val = high_val[valid_sym]\n",
    "low_val = low_val[valid_sym]\n",
    "close_val = close_val[valid_sym]\n",
    "volume = volume[valid_sym]\n",
    "next_val = next_val[valid_sym]\n",
    "ret_val = ret_val[valid_sym]\n",
    "hl_val = hl_val[valid_sym]\n",
    "\n",
    "# The valid symbols looks something like this..\n",
    "\n",
    "valid_sym\n",
    "\n",
    "Let's look at the average correlation to next day return values and rank the symbols accordingly.\n",
    "\n",
    "corr_val = pd.DataFrame()\n",
    "\n",
    "for Symbol in valid_sym:\n",
    "    df = pd.concat([open_val[Symbol], high_val[Symbol], low_val[Symbol], close_val[Symbol], volume[Symbol], next_val[Symbol], ret_val[Symbol], hl_val[Symbol]], axis=1, \n",
    "                   keys=[\"open\", \"high\", \"low\", \"close\", \"volume\", \"next_val\", \"ret_val\", \"hl_val\"])\n",
    "    corr_val = corr_val.append({\"symbol\": Symbol, \"avgcorr\": df.corr().drop(\"next_val\", axis = 1).loc['next_val'].abs().mean()}, ignore_index = True)\n",
    "\n",
    "# Histogram of values of average correlation to next day return\n",
    "corr_val.avgcorr.hist();\n",
    "plt.xlabel(\"Average correlation\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Average correlation to next day return\")\n",
    "plt.show()\n",
    "\n",
    "Based on the histogram above, we can see that there are very few symbols with good correlation to next day values. For high profitability, we are taking the symbols that possess good predictive power.\n",
    "Let's consider the tickers with average correlation of more than the 90%.\n",
    "\n",
    "sym90 = corr_val[corr_val.avgcorr > corr_val.avgcorr.quantile(0.90)].symbol.values\n",
    "\n",
    "open_val = open_val[sym90]\n",
    "high_val = high_val[sym90]\n",
    "low_val = low_val[sym90]\n",
    "close_val = close_val[sym90]\n",
    "volume = volume[sym90]\n",
    "next_val = next_val[sym90]\n",
    "ret_val = ret_val[sym90]\n",
    "hl_val = hl_val [sym90]\n",
    "\n",
    "# Modify original dataframes\n",
    "\n",
    "close_val = close_val.fillna(method=\"ffill\") # close\n",
    "close_val = close_val.fillna(method=\"bfill\") # to handle the first row of close\n",
    "volume = volume.applymap(lambda x: 0 if pd.isna(x) is True else x) # volume\n",
    "open_val = open_val.fillna(close_val)\n",
    "high_val = high_val.fillna(close_val)\n",
    "low_val = low_val.fillna(close_val)\n",
    "\n",
    "# calculate other dataframes\n",
    "next_val = (close_val.shift(-1) / close_val) - 1 # future return\n",
    "ret_val = (close_val / close_val.shift(1)) - 1 # return\n",
    "hl_val = high_val / low_val # high/low\n",
    "\n",
    "## Technical Analysis\n",
    "\n",
    "We want to create features that possess some level of predictive power which could indicate the future direction of the market. Statistically, these features should have good correlation  power to the market movement.\n",
    "\n",
    "There are many such transformations in Statistics. \n",
    "\n",
    "According to work by Borovkova, some of the key technical indicators that can be used are categorised into four groups, Momentum, Trend, Volume and Volatility. Some of the commonly used indicators are:\n",
    "\n",
    "#### Momentum:-\n",
    "\n",
    "    1. Money flow index \n",
    "    2. Relative strength index\n",
    "    3. Stochastic oscillator\n",
    "    4. William %R\n",
    "    \n",
    "#### Trend:-\n",
    "\n",
    "    1. Exponential moving average\n",
    "    2. Moving average convergence-divergence\n",
    "    3. Commodity channel index\n",
    "    4. Ichimoku Indicator\n",
    "    \n",
    "#### Volume:-\n",
    "\n",
    "    1. Accumulation/distribution index\n",
    "    \n",
    "#### Volatility:-\n",
    "\n",
    "    1. Bollinger bands \n",
    "\n",
    "# Technical Analysis package\n",
    "\n",
    "import ta\n",
    "\n",
    "##### 1. Money flow index\n",
    "\n",
    "import ta\n",
    "\n",
    "mfi = pd.DataFrame()\n",
    "\n",
    "for Symbol in close_val.columns:\n",
    "    \n",
    "    temp = ta.momentum.money_flow_index(high=high_val[Symbol], low=low_val[Symbol], close=close_val[Symbol], volume=volume[Symbol], fillna=True)\n",
    "    mfi = pd.concat([mfi, temp], axis=1,sort=True)\n",
    "\n",
    "# renaming the columns\n",
    "mfi.columns = close_val.columns\n",
    "\n",
    "##### 2. Relative Strength Index\n",
    "\n",
    "rsi = close_val.apply(ta.momentum.rsi, fillna=True)\n",
    "\n",
    "##### 3. Stochastic oscillator\n",
    "\n",
    "stoch_k = pd.DataFrame()\n",
    "stoch_d = pd.DataFrame()\n",
    "\n",
    "for Symbol in close_val.columns:\n",
    "    temp = ta.momentum.stoch(high=high_val[Symbol], low=low_val[Symbol], close=close_val[Symbol], fillna=True)\n",
    "    stoch_k = pd.concat([stoch_k, temp], axis=1,sort=True)\n",
    "    \n",
    "    temp = ta.momentum.stoch_signal(high=high_val[Symbol], low=low_val[Symbol], close=close_val[Symbol], fillna=True)\n",
    "    stoch_d = pd.concat([stoch_d, temp], axis=1,sort=True)\n",
    "\n",
    "# renaming the columns\n",
    "stoch_k.columns = close_val.columns\n",
    "stoch_d.columns = close_val.columns\n",
    "\n",
    "##### 4. William %R\n",
    "\n",
    "will_r = pd.DataFrame()\n",
    "\n",
    "for Symbol in close_val.columns:\n",
    "    temp = ta.momentum.wr(high=high_val[Symbol], low=low_val[Symbol], close=close_val[Symbol], fillna=True)\n",
    "    will_r = pd.concat([will_r, temp], axis=1,sort=True)\n",
    "\n",
    "# renaming the columns\n",
    "will_r.columns = close_val.columns\n",
    "\n",
    "##### 5. Exponential moving average\n",
    "\n",
    "ema = close_val.apply(ta.trend.ema_indicator, fillna=True)\n",
    "\n",
    "##### 6. Moving average convergence-divergence\n",
    "\n",
    "macd = close_val.apply(ta.trend.macd_diff, fillna=True)\n",
    "\n",
    "##### 7. Commodity channel index\n",
    "\n",
    "cci = pd.DataFrame()\n",
    "\n",
    "for Symbol in close_val.columns:\n",
    "    temp = ta.trend.cci(high=high_val[Symbol], low=low_val[Symbol], close=close_val[Symbol], fillna=True)\n",
    "    cci = pd.concat([cci, temp], axis=1, sort=True)\n",
    "\n",
    "# renaming the columns\n",
    "cci.columns = close_val.columns\n",
    "\n",
    "##### 8. Ichimoku Indicator\n",
    "\n",
    "ichi_a = pd.DataFrame()\n",
    "ichi_b = pd.DataFrame()\n",
    "\n",
    "for Symbol in close_val.columns:\n",
    "    temp = ta.trend.ichimoku_a(high=high_val[Symbol], low=low_val[Symbol], fillna=True)\n",
    "    ichi_a = pd.concat([ichi_a, temp], axis=1, sort=True)\n",
    "\n",
    "    temp = ta.trend.ichimoku_b(high=high_val[Symbol], low=low_val[Symbol], fillna=True)\n",
    "    ichi_b = pd.concat([ichi_b, temp], axis=1, sort=True)\n",
    "\n",
    "# renaming the columns\n",
    "ichi_a.columns = close_val.columns\n",
    "ichi_b.columns = close_val.columns\n",
    "\n",
    "##### 9. Accumulation/distribution index\n",
    "\n",
    "ad = pd.DataFrame()\n",
    "\n",
    "for Symbol in close_val.columns:\n",
    "    temp = ta.volume.acc_dist_index(high=high_val[Symbol], low=low_val[Symbol], close=close_val[Symbol], volume=volume[Symbol],fillna=True)\n",
    "    ad = pd.concat([ad, temp], axis=1, sort=True)\n",
    "\n",
    "# renaming the columns\n",
    "ad.columns = close_val.columns\n",
    "\n",
    "##### 10. Bollinger bands\n",
    "\n",
    "bb_up = close_val.apply(ta.volatility.bollinger_hband, fillna=True)\n",
    "bb_down = close_val.apply(ta.volatility.bollinger_lband, fillna=True)\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "# Listing all the above needed dataframes\n",
    "\n",
    "dfois = [open_val, high_val, low_val, close_val,volume, ret_val, hl_val,\n",
    "         mfi, ema, rsi, stoch_k, stoch_d, macd, will_r, cci,\n",
    "         ichi_a, ichi_b, ad, bb_up, bb_down, next_val]\n",
    "dfois_str = ['open', 'high', 'low', 'close', 'volume', 'rtn', 'hl','mfi', 'ema', 'rsi', 'stoch_k', 'stoch_d', 'macd', \n",
    "             'will_r', 'cci', 'ichi_a', 'ichi_b', 'ad', 'bb_up', 'bb_down', 'next_val']\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "corr_val = np.empty([len(dfois),len(dfois)])\n",
    "\n",
    "# Loop over every dataframe and find correlation\n",
    "\n",
    "for i, df1 in enumerate(dfois):\n",
    "    for j, df2 in enumerate(dfois):\n",
    "        corr_val[i][j] = df1.corrwith(df2).mean()\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "df_corr = pd.DataFrame(corr_val, columns=dfois_str, index=dfois_str)\n",
    "\n",
    "# Heatmap using seaborn\n",
    "\n",
    "sns.heatmap(df_corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(10, 240, n=500), ax=ax)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right');\n",
    "\n",
    "ax.set_title('Avg. colleration matrix heatmap');\n",
    "\n",
    "In additian to the correlation matrix heatmap, let's have a look at the histogram of data to look for outliers. we will normalise the data from 0 to 1 for every ticker so that they can be visualised and benchmarked appropriately.\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Normalise and draw subplots for each feature\n",
    "\n",
    "fig, axs = plt.subplots(10,2, figsize=(20, 30))   \n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    if i >= 20:\n",
    "        pass\n",
    "    else:\n",
    "        nomaliser = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "        df_val = nomaliser.fit_transform(dfois[i])\n",
    "        ax.hist(df_val.reshape(df_val.shape[0]*df_val.shape[1]), bins=50)\n",
    "        ax.set_title(dfois_str[i])\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.grid()\n",
    "\n",
    "Analysing the correlation heatmap and histograms above.\n",
    "\n",
    "Open, high, low and close values are highly correlated to each other. This is expected because the values are within close proximity to each other. As the technical indicators are calculated using these values and intrinsicly retain its information, open, high, and low values can be removed from feature list.\n",
    "\n",
    "Most of the distributions above are either normal or uniform in shape, expect for hl and volume. This suggests that there will be a large number of outliers in those two features. Although the other features with normal distribution may have outliers in the tail section, they can be considered negligible. In addition, observing the correlation matrix, they also appear to have low correlation to future return, which is the metric we will be predicting. Therefore, it justifies to leave hl and volume out from the feature list.\n",
    "Similarly, based on the correlation matrix heatmap, some values appear to not have much correlation to next day value. They are: mfi and ad. To keep the prediction model simple, these features can also be regarded as not useful.\n",
    "\n",
    "Finally, it also appears that stoch_k is highly correlated to william_r. This is expected because the mathematical expression of both the indicators are similar. Here, william_r will be removed from the feature list.\n",
    "\n",
    "Based on the discussion above, the final feature list would be: close, rtn, ema, rsi, stoch_k, stoch_d, macd, cci, ichi_a, ichi_b, bb_up, bb_down.\n",
    "\n",
    "Our strategy is to predict the UP and DOWN movement of a stock together. Based on experience, this is a better strategy compared to predicting the direction of future return as the performance metric based on this can be misleading. For example, if we build a model that minimises the mean-squared-error, that still doesn't mean that the direction of the movement can be correct. If the actual movement is 0.1%, then investing based on 0.5% is better compared to -0.1% prediction although the later might have a error value.\n",
    "\n",
    "Now, we first need to create a target dataframe that categorises into 1 and 0: 1 for UP and 0 for DOWN. Then, let us look at the distribution of the values.\n",
    "\n",
    "# Create target dataframe\n",
    "df_target = (next_val > 0).astype(int)\n",
    "\n",
    "# Look at the proportion\n",
    "values, counts = np.unique(df_target.values.reshape(df_target.shape[0]*df_target.shape[1]), return_counts=True)\n",
    "plt.bar(values,counts,tick_label=['DOWN','UP'])\n",
    "plt.title('Number of UPs and DOWNs')\n",
    "plt.ylabel('Total number')\n",
    "plt.xlabel('Categories')\n",
    "plt.show()\n",
    "\n",
    "# List all the dataframes of interest\n",
    "dfois = [close_val, ret_val, ema, rsi, stoch_k, stoch_d, macd, cci, ichi_a, ichi_b, bb_up, bb_down, df_target]\n",
    "dfois_str = ['close', 'rtn', 'ema', 'rsi', 'stoch_k', 'stoch_d', 'macd', 'cci','ichi_a', 'ichi_b', 'bb_up', 'bb_down', 'target']\n",
    "\n",
    "Based on the analysis above, it appears that there are more DOWNs than UPs. Therefore, some balancing is required before sending the data for machine learning. Before going into forecasting section, the final list of dataframes should be defined based on the features list finalised earlier.\n",
    "\n",
    "## Prediction\n",
    "\n",
    "##### Pre-processing the data\n",
    "\n",
    "Based on the Data Analysis done above, this section builds an LSTM neural network model for the prediction of the next day stock price moment; whether it is going upward or downward.\n",
    "\n",
    "First, let's split the data to training, validation, and test sets. The training and validation sets will be using during the LSTM network training, while the test set will be used for trading strategy implementation and additional testing of the final model.\n",
    "\n",
    "# Train set\n",
    "dfois_train = []\n",
    "for df in dfois:\n",
    "    dfois_train.append(df.iloc[df.index < '2019-03-01'])\n",
    "\n",
    "# Test set\n",
    "dfois_test = []\n",
    "for df in dfois:\n",
    "    dfois_test.append(df.iloc[df.index >= '2019-08-01'])\n",
    "\n",
    "# Validation set\n",
    "dfois_eval = []\n",
    "for df in dfois:\n",
    "    dfois_eval.append(df.iloc[(df.index >= '2019-03-01') & (df.index < '2019-08-01')])\n",
    "\n",
    "Next, let us preprocess the data. Let us normalise all the data to be between 0 and 1. Note that the maximum and minimum is with respect to each ticker in each train dataframe\n",
    "\n",
    "# List of normalisers corresponding to each dataframe\n",
    "nomalisers = []\n",
    "\n",
    "for i, df in enumerate(dfois[:-1]):\n",
    "    # create the normaliser object\n",
    "    nomalisers.append(preprocessing.MinMaxScaler(feature_range=(0,1)))\n",
    "    \n",
    "    # columns and indexes\n",
    "    columns = dfois_train[i].columns\n",
    "    index_train = dfois_train[i].index\n",
    "    index_test = dfois_test[i].index\n",
    "    index_eval = dfois_eval[i].index\n",
    "    \n",
    "    # fit normalise\n",
    "    nomalisers[i].fit(dfois_train[i])\n",
    "    \n",
    "    # trasform\n",
    "    train_data = nomalisers[i].transform(dfois_train[i])\n",
    "    eval_data = nomalisers[i].transform(dfois_eval[i])\n",
    "    test_data = nomalisers[i].transform(dfois_test[i])\n",
    "    \n",
    "    # replace list\n",
    "    dfois_train[i] = pd.DataFrame(train_data, columns=columns, index=index_train)\n",
    "    dfois_eval[i] = pd.DataFrame(eval_data, columns=columns, index=index_eval)\n",
    "    dfois_test[i] = pd.DataFrame(test_data, columns=columns, index=index_test)\n",
    "\n",
    "Now, let us scale the data to have mean 0 and unit variance. This scaling is done by training the scaler using data of entire normalised training dataframe. This will be used to transform the training, validation, and test sets.\n",
    "\n",
    "# List of scalers corresponding to each dataframe\n",
    "scalers = []\n",
    "\n",
    "# Standardise the data by fitting the train set\n",
    "for i, _ in enumerate(dfois[:-1]):\n",
    "    # create the Scaler object\n",
    "    scalers.append(preprocessing.StandardScaler())\n",
    "    \n",
    "    # columns and indexes\n",
    "    columns = dfois_train[i].columns\n",
    "    index_train = dfois_train[i].index\n",
    "    index_test = dfois_test[i].index\n",
    "    index_eval = dfois_eval[i].index\n",
    "    \n",
    "    # fit scale\n",
    "    flat_arr = dfois_train[i].values.reshape(dfois_train[i].shape[0]*dfois_train[i].shape[1],1)\n",
    "    scalers[i].fit(np.tile(flat_arr, dfois_train[i].shape[1]))\n",
    "    \n",
    "    # trasform\n",
    "    train_data = scalers[i].transform(dfois_train[i])\n",
    "    eval_data = scalers[i].transform(dfois_eval[i])\n",
    "    test_data = scalers[i].transform(dfois_test[i])\n",
    "    \n",
    "    # replace list\n",
    "    dfois_train[i] = pd.DataFrame(train_data, columns=columns, index=index_train)\n",
    "    dfois_eval[i] = pd.DataFrame(eval_data, columns=columns, index=index_eval)\n",
    "    dfois_test[i] = pd.DataFrame(test_data, columns=columns, index=index_test)\n",
    "\n",
    "##### 60 Time Steps\n",
    "\n",
    "Let's sequence the data. We are going to use the past 60 days of data for the next day prediction, we need to append the past 60 days worth data and append them in an array. We first write a function and execute onto all three sets of data.\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Look at the past 60 days\n",
    "SEQ_LEN = 60\n",
    "\n",
    "def sequence_data(df_list, shuffle=True):\n",
    "    # list containing the data\n",
    "    sequential_data = []\n",
    "\n",
    "    for Symbol in close_val.columns:\n",
    "        # initialise dataframe\n",
    "        df_ticker = pd.DataFrame()\n",
    "\n",
    "        # concatenate the dataframes\n",
    "        for df in df_list:\n",
    "            df_ticker = pd.concat([df_ticker, df[Symbol]], axis=1)\n",
    "\n",
    "        prev_days = deque(maxlen=SEQ_LEN)\n",
    "        # for values in every row\n",
    "        for i in df_ticker.values: \n",
    "            # remove the targets\n",
    "            prev_days.append([n for n in i[:-1]])  \n",
    "            # append when sequence length is reached\n",
    "            if len(prev_days) == SEQ_LEN: \n",
    "                sequential_data.append([np.array(prev_days), i[-1]])\n",
    "                \n",
    "        # shuffle - we do not need to do this for test set\n",
    "        if shuffle == True:\n",
    "            random.shuffle(sequential_data)\n",
    "\n",
    "    return sequential_data\n",
    "\n",
    "sequential_data_train = sequence_data(dfois_train)\n",
    "sequential_data_eval = sequence_data(dfois_eval)\n",
    "sequential_data_test = sequence_data(dfois_test, shuffle=False) # do not shuffle just this one\n",
    "\n",
    "# Print the length\n",
    "print('Training data length: {}'.format(len(sequential_data_train)))\n",
    "print('Validation data length: {}'.format(len(sequential_data_eval)))\n",
    "print('Testing data length: {}'.format(len(sequential_data_test)))\n",
    "\n",
    "# balance train and evaluation data\n",
    "def balance_data(sequential_data):\n",
    "    ups = [] \n",
    "    downs = [] \n",
    "    \n",
    "    # separate the sequence into ups and downs\n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            downs.append([seq, target])\n",
    "        elif target == 1:\n",
    "            ups.append([seq, target])\n",
    "    \n",
    "    # shuffle to randomise\n",
    "    random.shuffle(ups)\n",
    "    random.shuffle(downs)\n",
    "    \n",
    "    # get the shorter length\n",
    "    lower = min(len(ups), len(downs))\n",
    "    \n",
    "    # truncate the list to shorter length\n",
    "    ups = ups[:lower]\n",
    "    downs = downs[:lower]\n",
    "    \n",
    "    # merge and shuffle\n",
    "    sequential_data = ups+downs\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    return sequential_data\n",
    "\n",
    "# separate train and target data\n",
    "def separate_data(sequential_data):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # loop over every row in sequential data\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    return np.array(X), y\n",
    "\n",
    "# perform balancing by calling the function\n",
    "train_x, train_y = separate_data(balance_data(sequential_data_train))\n",
    "validation_x, validation_y = separate_data(balance_data(sequential_data_eval))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ModelCheckpoint\n",
    "\n",
    "# Batch size and epochs\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 3\n",
    "\n",
    "# Build LSTM prediction model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.001, decay=1e-6),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(validation_x, validation_y))\n",
    "\n",
    "# save the model\n",
    "model.save(\"lstm.l1\")\n",
    "\n",
    "# load the model\n",
    "model = load_model(\"lstm.l1\")\n",
    "\n",
    "Let's look at the accuracy based on the test dataset as well as the classification report based on the 2018/2019 data which the model has never seen during training.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# test performance using test set\n",
    "test_x, test_y = separate_data(sequential_data_test)\n",
    "\n",
    "# get the prediction\n",
    "pred = model.predict_classes(test_x)\n",
    "\n",
    "# get prediction probability\n",
    "pred_proba = model.predict(test_x)\n",
    "\n",
    "# accuracy using test\n",
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('')\n",
    "\n",
    "# classification report\n",
    "print('Classification report:')\n",
    "print(classification_report(test_y, pred))\n",
    "\n",
    "## Trading strategy\n",
    "\n",
    "Let us define a simple trading strategy. Of all 188 tickers used, open position using $1 for each ticker based on the predicted movement by the LSTM network, and close position the next day. This means that everyday, an investment of $150 is made and position must be closed the next day.\n",
    "\n",
    "Note that, as the LSTM network requires 60 days of past data, the trading is simulated from 2019-08-01. The cumulative profit and loss (PnL) will be plotted.\n",
    "\n",
    "The accuracy score based on the test set shows that the results are in line with research whereby the accuracy on test set is lower than that of the training and validation set. The prediction of upward movement has low precision compared to that of the downward trend, which means that there are more false positives when predicting the upward movement. Often, the recall score is inversely proportional to the precision. Therefore, the best measure is the f1-score, which is acceptable for developing a trading strategy.\n",
    "\n",
    "# build index list\n",
    "pred_index = dfois_test[0].index[dfois_test[0].index >= dfois_test[0].head(60).index[-1]]\n",
    "\n",
    "# build prediction dataframe - 1 is BUY and 0 is SELL\n",
    "df_pred = pd.DataFrame(pred.reshape(dfois_test[0].shape[0]-SEQ_LEN+1,dfois_test[0].shape[1], order='F'),\n",
    "                       index = pred_index,\n",
    "                       columns = dfois_test[0].columns)\n",
    "\n",
    "# dataframe for right prediction - 1 is CORRECT and 0 is WRONG\n",
    "df_right = (df_pred.astype(bool) == df_target.loc[pred_index].astype(bool)).astype(int)\n",
    "\n",
    "# dataframe for wrong prediction - 1 is WRONG and 0 is CORRECT\n",
    "df_wrong = (~df_right.astype(bool)).astype(int)\n",
    "\n",
    "# datframe for profit\n",
    "df_profit = df_right*next_val.loc[pred_index].abs()\n",
    "\n",
    "# dataframe for loss\n",
    "df_loss = df_wrong*next_val.loc[pred_index].abs()\n",
    "\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.plot((df_profit.sum(axis=1) - df_loss.sum(axis=1)).cumsum())\n",
    "plt.grid()\n",
    "plt.xlabel('Trading dates')\n",
    "plt.ylabel('Cumulative PnL (AUD)')\n",
    "plt.title('Graph of cumulative profit and loss (PnL)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# pnl of lstm\n",
    "pnl_lstm = (df_profit.sum(axis=1) - df_loss.sum(axis=1))\n",
    "\n",
    "# plotting the roi per day\n",
    "plt.subplots(figsize=(16, 24))\n",
    "plt.barh((pnl_lstm/150*100).index,(pnl_lstm/150*100).values, label = \"LSTM\", align='edge', height=0.25)\n",
    "plt.xlabel('ROI per day (%)')\n",
    "plt.ylabel('Trading dates')\n",
    "plt.title('Graph of return on investment (%) per day for different strategies')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "Following the simple strategy, based on 40 days of trading and  188 ğ‘–ğ‘›ğ‘£ğ‘’ğ‘ ğ‘¡ğ‘šğ‘’ğ‘›ğ‘¡ ğ‘ğ‘’ğ‘Ÿğ‘‘ğ‘ğ‘¦, ğ‘¡â„ğ‘’ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘ğ‘Ÿğ‘œğ‘“ğ‘–ğ‘¡ ğ‘šğ‘ğ‘‘ğ‘’ ğ‘–ğ‘  ğ‘ğ‘ğ‘œğ‘¢ğ‘¡ 6. Considering the numbers and the linear trend of the cumulative PnL, I would say that this is a successful strategy.\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1, figsize=(15, 5), sharey=True)   \n",
    "\n",
    "ax1.hist((pnl_lstm/188*100).values, bins=10)\n",
    "ax1.set_title(\"LSTM\")\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_xlabel('ROI (%)')\n",
    "ax1.grid()\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# calculating the confidence\n",
    "conf_lstm = stats.percentileofscore((pnl_lstm/188*100).values, 2) - stats.percentileofscore((pnl_lstm/188*100).values, 0)\n",
    "\n",
    "print('Confidence of employed approaches in making positive ROI based on test set:')\n",
    "print('LSTM: {}%'.format(conf_lstm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
